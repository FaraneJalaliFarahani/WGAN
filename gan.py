# -*- coding: utf-8 -*-
"""GAN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1B52h-ctlspK_J55SZcD-GEhTzuAcNdk8
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms

# MNIST Dataset
original_train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)
original_test_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor(), download=True)

# Device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Set Hyper-parameters (change None)
BATCH_SIZE = 100
LEARNING_RATE_D = 0.0002
LEARNING_RATE_G = 0.0002
N_EPOCH = 80

# Define Train loader
train_tensors = original_train_dataset.data.float() / 255
test_tensors = original_test_dataset.data.float() / 255

train_dataset = torch.utils.data.TensorDataset(train_tensors, original_train_dataset.targets)
test_dataset = torch.utils.data.TensorDataset(test_tensors, original_test_dataset.targets)

train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)
test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)

latent_size = 128
hidden_size = 256
image_size = 784

discriminator = nn.Sequential(
    nn.Dropout(),
    nn.Linear(image_size, hidden_size),
    nn.LeakyReLU(0.2),
    nn.Linear(hidden_size, hidden_size),
    nn.LeakyReLU(0.2),
    nn.Linear(hidden_size, 1),
    nn.Sigmoid()
)

generator = nn.Sequential(

    nn.Linear(latent_size, hidden_size),
    nn.LeakyReLU(0.2),
    nn.Linear(hidden_size, hidden_size),
    nn.LeakyReLU(0.2),
    nn.Linear(hidden_size, image_size),
    nn.Sigmoid()

)

# Device setting
discriminator = discriminator.to(device)
generator= generator.to(device)

# Create two optimizer for discriminator and generator (change None), you can also change the optimizer
opt_D = optim.Adam(discriminator.parameters(), lr=LEARNING_RATE_D)
opt_G = optim.Adam(generator.parameters(), lr=LEARNING_RATE_G)
criterion = nn.BCELoss()

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline
plt.rcParams['figure.figsize'] = (10, 3) # set default size of plots

for epoch in range(N_EPOCH):
    for i, (img, label) in enumerate(train_loader):
        
        img = img.reshape(img.shape[0], -1)
        real_img = img.to(device)

        fake_labels = torch.zeros(img.shape[0], 1).to(device)
        real_labels = torch.ones(img.shape[0], 1).to(device)

        # Compute BCE_Loss using real images where BCE_Loss(x, y): - y * log(D(x)) - (1-y) * log(1 - D(x))
        discriminator.zero_grad()
        outputs=discriminator(real_img)
        loss_d_real = criterion(outputs, real_labels)
        # Compute BCELoss using fake images        
        z = torch.randn(img.shape[0], 128).to(device)
        fake_img = generator(z)
        outputs=discriminator(fake_img)
        loss_d_fake = criterion(outputs, fake_labels)
        opt_D.zero_grad()
        loss_d = loss_d_fake + loss_d_real
        loss_d.backward()
        opt_D.step()
        # to train G, maximize log(D(G(z)) instead of minimizing log(1-D(G(z)))
        generator.zero_grad()
        z = torch.randn(img.shape[0], 128).to(device)
        fake_img = generator(z)
        outputs= discriminator(fake_img)
        
        loss_g =  criterion(outputs, real_labels)
        loss_g.backward()
        opt_G.step()

    
    print("epoch: {} \t last batch loss D: {} \t last batch loss G: {}".format(epoch + 1, 
                                                                               loss_d.item(), 
                                                                               loss_g.item()))

    for i in range(3):
        for j in range(10):
            plt.subplot(3, 10, i * 10 + j + 1)
            plt.imshow(fake_img[i * 10 + j].detach().cpu().view(28, 28).numpy())
    plt.show()